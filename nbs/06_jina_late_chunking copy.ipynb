{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastcore.xtras import *\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework.documents import load_docling\n",
    "from docling_core.types.doc import ImageRefMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jina Late Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import numpy as np\n",
    "\n",
    "# model_name = \"Alibaba-NLP/gte-modernbert-base\"\n",
    "model_name = \"nomic-ai/modernbert-embed-base\"\n",
    "# model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "\n",
    "MAX_LEN = 8192\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "enc_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the document\n",
    "doc = load_docling(Path(\"scratch/sample_doc.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: removing image placeholders in the document for now\n",
    "# text = doc.export_to_markdown(image_mode=ImageRefMode.PLACEHOLDER, image_placeholder=\"</IMAGE>\")\n",
    "text = doc.export_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docling_core.types.doc.document.DoclingDocument"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how long the document is\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors='pt',\n",
    "    return_offsets_mapping=True,\n",
    "\n",
    "    # NOTE: we are passing in the first chunk\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "); len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8192])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checking that transformers and sentence transformers are giving the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 768), numpy.ndarray)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = enc_model.encode([text[:100]])\n",
    "e1.shape, type(e1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Tensor)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tokenizer(text[:100], return_tensors='pt')\n",
    "e2 = model(**t2)\n",
    "e2 = mean_pooling(e2, t2['attention_mask'])\n",
    "e2.shape, type(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1\n",
    "e2\n",
    "cos_sim(e1, e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping back from token ids in the inputs to the original text, so we know what's what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(656)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull out a sample token from the encoded inputs\n",
    "token_ids = inputs['input_ids'][0]\n",
    "_sample_id = 100\n",
    "_token_one_id = token_ids[_sample_id]\n",
    "_token_one_id # <- This is the token id at this place in encoded inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([340, 342])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the mappings, we can find out where in the text this token is\n",
    "token_offsets = inputs['offset_mapping'][0]\n",
    "_token_offset = token_offsets[_sample_id]\n",
    "_token_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ys'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sure enough, we can index into the original text with the offset and be anchored in our context\n",
    "region = text[\n",
    "    _token_offset[0]:_token_offset[1]\n",
    "]; region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this should agree with the token_id we pulled earlier.\n",
    "tokenizer.vocab[region]; assert tokenizer.vocab[region] == _token_one_id.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how long the document is\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors='pt',\n",
    "\n",
    "    # NOTE: we are passing in the first chunk\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "); len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8192])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can embed each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8192, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = model(**inputs)\n",
    "token_embeddings = model_output[0]\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each of these represents a word's meaning in the context of this document, not just an isolated meaning of the word itself. That's the genius of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking the doc-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do this by periods in the token space, with the added benefit that now each token includes the context of the previous tokens\n",
    "punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "punctuation_mark_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use period + space to get the start of a chunk. We're basically doing sentence encoding here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_pos, token_spans = [], []\n",
    "\n",
    "span_start_char, span_start_token = 0, 0 \n",
    "\n",
    "for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\n",
    "    # iterate through the tokens\n",
    "    if i < len(token_ids) - 1:\n",
    "        # check if we have a period, followed by a space or newline. \n",
    "        if token_id == punctuation_mark_id and text[end:end+1] in [' ', '\\n']:\n",
    "            # store both char positions, and token start and ends\n",
    "            chunk_pos.append(\n",
    "                (span_start_char, int(end))\n",
    "            )\n",
    "            token_spans.append(\n",
    "                (span_start_token, i+1)\n",
    "            )\n",
    "\n",
    "            # update pos for the next chunk\n",
    "            span_start_char, span_start_token = int(end)+1, i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out some of the chunks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "Char span: (0:960): Article\n",
      "\n",
      "## DNA Damage in Moderate and Severe COVID-19 Cases: Relation to Demographic, Clinical, and Laboratory Parameters\n",
      "\n",
      "Kalashyan 1D , Naira Stepanyan (D) Hovhannisyan 1,2 @ Lily\n",
      "\n",
      "- Laboratory of General and Molecular Genetics , Research Institute of Yerevan State University, Alex Manoogian 1, Yerevan 0025, Armenia; tigranharutyunyan@ysu.am (TH); angela.sargsyan@ysu.am (A.S.); lilikalashyan@ysuam (LK.); genetik@ysu.am (RA.); galinahovhannisyan@ysu.am (G.H:) Biology\n",
      "- Department of Genetics and Cytology; Yerevan State University, Alex Manoogian 1, Yerevan 0025 , Armenia\n",
      "- National Center for Infectious Diseases , Arno Babajanyan 21, Yerevan 0064, Armenia; nsstepanyang@gmailcom\n",
      "- Jena University Hospital, Institute of Human Genetics, Friedrich Schiller University, Am Klinikum 1, D-07747 Jena, Germany\n",
      "\n",
      "Abstract: The of the SARS-CoV-2 virus to cause DNA damage in infected humans requires its study as a potential indicator of COVID-19 progression.\n",
      "Chunk: 1\n",
      "Char span: (961:1131): DNA damage was studied in leukocytes of 65 COVID-19 patients stratified by sex, age, and disease severity in relation to demographic, clinical, and laboratory parameters.\n",
      "Chunk: 2\n",
      "Char span: (1132:1240): In combined group of COVID-19 patients, DNA damage was shown to be elevated compared to controls (12.44% vs.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    char_start, char_end = chunk_pos[i]\n",
    "    tok_start, tok_end = token_spans[i]\n",
    "\n",
    "    print(f\"Chunk: {i}\")\n",
    "    print(f'Char span: ({char_start}:{char_end}): {text[char_start:char_end].strip(\"\\n\")}')\n",
    "    # print(f'Token span: ({tok_start}:{tok_end}): {token_ids[tok_start:tok_end]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use our chunks in token space to chunk the token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([297, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token, end_token = token_spans[0]\n",
    "chunk_embedding = token_embeddings[0, start_token:end_token]\n",
    "chunk_embedding.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many tokens are in the first chunk, and we have an embedding for each token. But, we need to \"average\" these down to get a single embedding for the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool the embeddings\n",
    "chunk_embedding = chunk_embedding.mean(dim=0)\n",
    "chunk_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "# for each token span, calculate the mean of its token embeddings\n",
    "for start, end in token_spans:\n",
    "    if end > start: # ensure span has at least one token\n",
    "        # mean pool the token embeddings for this chunk\n",
    "        chunk_embed = token_embeddings[0, start:end].mean(dim=0)\n",
    "        embeddings.append(chunk_embed)\n",
    "\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a function for it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(document, model, tokenizer):\n",
    "    \"Implements late chunking on a document.\"\n",
    "\n",
    "    # Tokenize with offset mapping to find sentence boundaries\n",
    "    inputs_with_offsets = tokenizer(\n",
    "        document,\n",
    "        return_tensors='pt',\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    token_offsets = inputs_with_offsets['offset_mapping'][0]\n",
    "    token_ids = inputs_with_offsets['input_ids'][0]\n",
    "    \n",
    "    # Find chunk boundaries\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')    \n",
    "    chunk_positions, token_span_annotations = [], []\n",
    "    span_start_char, span_start_token = 0, 0\n",
    "\n",
    "    for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\n",
    "        if i < len(token_ids)-1:\n",
    "            if token_id == punctuation_mark_id and document[end:end+1] in [' ', '\\n']:\n",
    "                # Store both character positions and token positions\n",
    "                chunk_positions.append((span_start_char, int(end)))\n",
    "                token_span_annotations.append((span_start_token, i+1))\n",
    "                \n",
    "                # Update start positions for next chunk\n",
    "                span_start_char, span_start_token = int(end)+1, i+1\n",
    "    \n",
    "    # Create text chunks from character positions\n",
    "    chunks = [document[start:end].strip() for start, end in chunk_positions]\n",
    "    \n",
    "    # Encode the entire document\n",
    "    inputs = tokenizer(\n",
    "        document,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    model_output = model(**inputs)\n",
    "    token_embeddings = model_output[0]\n",
    "    \n",
    "    # Create embeddings for each chunk using mean pooling\n",
    "    embeddings = []\n",
    "    for start_token, end_token in token_span_annotations:\n",
    "        if end_token > start_token:  # Ensure span has at least one token\n",
    "            chunk_embedding = token_embeddings[0, start_token:end_token].mean(dim=0)\n",
    "            embeddings.append(chunk_embedding.detach().cpu().numpy())\n",
    "\n",
    "    embeddings = np.stack(embeddings)\n",
    "    \n",
    "    return chunks, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings\n",
    "late_chunks, late_embeds = late_chunking(text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matched_late_retrieval(query, chunks, chunk_embeddings, top_k=3):\n",
    "    \"\"\"Retrieve the most relevant chunk for a query.\"\"\"\n",
    "\n",
    "    # embed the query, pooling as we did the chunks\n",
    "    query_tokens = tokenizer(query, return_tensors='pt')\n",
    "    query_embeddings = model(**query_tokens)\n",
    "    query_embedding = mean_pooling(query_embeddings, query_tokens['attention_mask'])\n",
    "    \n",
    "    # find similarities between query and chunks\n",
    "    similarities = cos_sim(query_embedding, chunk_embeddings).detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    # sort the most similar chunks\n",
    "    top_idx = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    # get the top chunks and their similarities\n",
    "    top_chunks = [chunks[i] for i in top_idx]\n",
    "    top_sims = [similarities[i] for i in top_idx]\n",
    "    \n",
    "    return top_chunks, top_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample saving to file\n",
    "embeds_arr, chunks_arr = np.array(late_embeds), np.array(late_chunks)\n",
    "np.savez(\"scratch/late_embeds.npz\", embeds=embeds_arr, chunks=chunks_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, sims = matched_late_retrieval(\n",
    "    \"Was COVID more severe in men or women?\",\n",
    "    late_chunks,\n",
    "    late_embeds,\n",
    "    top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The only difference found between men and women with severe COVID-19 was that the procalcitonin (inflammation index) was higher in men than in women (Table 3).\\n\\nTable 3.', 'Men with severe versus moderate illness had higher BMI and CRP: Women with severe versus moderate disease had higher INR levels and longer hospital stays.', 'Thus, the sexual composition of the two groups was not significantly different.', 'In the group of severely ill patients , 55.2% were women and 44.8% were men.', 'WBC, white blood cells; NEU , neutrophils; LYM, lymphocytes; NLR, neutrophil to lymphocyte ratio; PLT, platelets; CRP; C-reactive protein; PCT, procalcitonin; INR, international nor malized ratio; APTT, activated thromboplastin time; ALT, alanine transaminase; AST, aspartate transferase; LOS, length of hospital stay partial\\n\\nLaboratory parameters were analyzed in COVID-19 patients in the context of age- and sex-related changes (Tables 2 and 3) Both men and women with COVID-19 were older in the severe than in the moderate group.']\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look into the specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_subset = ' '.join(late_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DNA damage was studied in leukocytes of 65 COVID-19 patients stratified by sex, age, and disease severity in relation to demographic, clinical, and laboratory parameters.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Late Chunking Works\n",
    "Late chunking solves the lost context problem in several important ways:\n",
    "\n",
    "Bidirectional context awareness: Each token embedding is influenced by all other tokens in the document, both before and after it. This means references like \"the city\" can be properly linked to \"Berlin\" mentioned earlier.\n",
    "\n",
    "Consistent representation: All chunks from the same document share the same contextual foundation, ensuring that related concepts are represented similarly regardless of which chunk they appear in.\n",
    "\n",
    "Preservation of long-range dependencies: Information from the beginning of a document can influence the representation of content at the end, maintaining semantic connections across the entire text.\n",
    "\n",
    "Resilience to boundary selection: Since each token's embedding already contains document-wide context, the specific chunking boundaries become less critical. This means simpler chunking strategies can work just as well as complex ones.\n",
    "\n",
    "The Importance of Long-Context Models\n",
    "Late chunking requires embedding models that can handle long contexts—ideally 8K tokens or more. These models aren't just standard embedding models with longer input windows; they're specifically designed to maintain coherent representations across thousands of tokens.\n",
    "\n",
    "The key advantages of these long-context models for late chunking include:\n",
    "\n",
    "Attention across the entire document: They can attend to relationships between distant parts of the text\n",
    "Training on document-level tasks: They're often fine-tuned on tasks that require understanding document structure\n",
    "Optimized pooling strategies: They use pooling methods that effectively compress long sequences\n",
    "Without these capabilities, late chunking wouldn't be possible or effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
